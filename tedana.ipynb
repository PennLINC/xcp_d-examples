{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fMRIPrep + tedana + XCP-D\n",
    "\n",
    "There are three key elements to running tedana + XCP-D:\n",
    "\n",
    "1.  XCP-D is very strict about how it expects external (i.e., custom) confounds files to be organized and named.\n",
    "    We need to copy the associated tedana derivatives (i.e., the mixing matrices) to a new folder and rename them,\n",
    "    so that XCP-D can find them.\n",
    "2.  Tedana's ICA will probably look bad if dummy scans haven't been dropped from the BOLD file.\n",
    "    See [this discussion](https://github.com/ME-ICA/tedana/discussions/899).\n",
    "    Therefore, we must drop the dummy scans before running tedana, \n",
    "    then buffer the mixing matrix produced by tedana with dummy data to fill in those scans,\n",
    "    and finally run XCP-D with the same number of dummy scans flagged, \n",
    "    so those volumes will ultimately be ignored.\n",
    "3.  Simply regressing out \"bad\" components flagged by tedana is a bad idea.\n",
    "    This is known as \"aggressive\" denoising, and is not recommended,\n",
    "    as \"bad\" components may share variance with \"good\" components that we are confident reflect real signals.\n",
    "    For more information on this, \n",
    "    see [tedana's documentation](https://tedana.readthedocs.io/en/latest/denoising.html).\n",
    "    You have two solid options for handling this:\n",
    "    \n",
    "    1.  Use the `--tedort` flag when running tedana, \n",
    "        so that \"bad\" components are orthogonalized with respect to \"good\" components.\n",
    "        Unless you have manually reviewed your components and are very sure that none of the \n",
    "        \"good\" components reflect noise \n",
    "        (including BOLD-based noise that tedana, by its nature, cannot distinguish from real BOLD signal),\n",
    "        then we recommend using this approach, and this is the one we document in this notebook.\n",
    "        See [this NeuroStars post](https://neurostars.org/t/summary-report-of-xcp-d-using-multi-echo-bold-and-tedana/28304)\n",
    "    2.  Allow XCP-D to orthogonalize _all_ of your nuisance regressors \n",
    "        (i.e., the tedana \"bad\" components and any additional confounds you choose to include in your XCP-D run) \n",
    "        with respect to the tedana \"good\" components.\n",
    "        The potential issue with this is if your tedana results classify some noise signals as \"good\" components \n",
    "        (e.g., if there is a misclassification or if some components reflect BOLD-based noise, \n",
    "        which tedana cannot distinguish from BOLD-based signals)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Run fMRIPrep 22.0.0+ with `--me-output-echos` flag\n",
    "\n",
    "This has already been handled for this dataset. Unfortunately, the data we use here are not publicly available yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Remove dummy scans from fMRIPrep files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tedana.workflows import tedana_workflow\n",
    "\n",
    "\n",
    "def _flag_dummyvols(confounds_file):\n",
    "    \"\"\"Identify the number of dummy volumes flagged in an fMRIPrep confounds file.\"\"\"\n",
    "    confounds_df = pd.read_table(confounds_file)\n",
    "    nss_cols = [c for c in confounds_df.columns if c.startswith(\"non_steady_state_outlier\")]\n",
    "    if nss_cols:\n",
    "        initial_volumes_df = confounds_df[nss_cols]\n",
    "        dummy_vols = np.any(initial_volumes_df.to_numpy(), axis=1)\n",
    "        dummy_vols = np.where(dummy_vols)[0]\n",
    "\n",
    "        # reasonably assumes all NSS volumes are contiguous\n",
    "        n_dummy_vols = int(dummy_vols[-1] + 1)\n",
    "        # dummy_scans = 10\n",
    "    else:\n",
    "        n_dummy_vols = 0\n",
    "    \n",
    "    return n_dummy_vols\n",
    "\n",
    "\n",
    "def _remove_dummyvols(in_file, out_file, n_dummy_vols):\n",
    "    \"\"\"Remove dummy volumes from in_file and write out to out_file.\n",
    "    \n",
    "    If n_dummy_vols is 0, then just return out_file.\n",
    "    \"\"\"\n",
    "    if n_dummy_vols:\n",
    "        print(f\"Dropping {n_dummy_vols} volumes from {os.path.basename(in_file)}\")\n",
    "\n",
    "        img = nib.load(in_file)\n",
    "        img = img.slicer[..., n_dummy_vols:]\n",
    "        img.to_filename(out_file)\n",
    "    else:\n",
    "        out_file = in_file\n",
    "\n",
    "    return out_file\n",
    "\n",
    "\n",
    "def drop_dummy_vols(bold_files, confounds_file, temp_dir=\".\"):\n",
    "    \"\"\"Remove dummy volumes from a list of files.\n",
    "    \n",
    "    This infers the number of dummy volumes from the confounds file.\n",
    "    The shortened files are written out to temp_dir with the same\n",
    "    filenames as the original files.\n",
    "    \"\"\"\n",
    "    shortened_files = []\n",
    "    n_dummy_vols = _flag_dummyvols(confounds_file)\n",
    "    for bold_file in bold_files:\n",
    "        if n_dummy_vols:\n",
    "            temp_file = os.path.join(temp_dir, os.path.basename(bold_file))\n",
    "            shortened_file = _remove_dummyvols(bold_file, temp_file, n_dummy_vols)\n",
    "        else:\n",
    "            shortened_file = bold_file\n",
    "        shortened_files.append(shortened_file)\n",
    "\n",
    "    return shortened_files, n_dummy_vols\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Run tedana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping 2 volumes from sub-06_ses-2_task-frackack_acq-multiecho_echo-1_part-mag_desc-preproc_bold.nii.gz\n",
      "Dropping 2 volumes from sub-06_ses-2_task-frackack_acq-multiecho_echo-2_part-mag_desc-preproc_bold.nii.gz\n",
      "Dropping 2 volumes from sub-06_ses-2_task-frackack_acq-multiecho_echo-3_part-mag_desc-preproc_bold.nii.gz\n",
      "Dropping 2 volumes from sub-06_ses-2_task-frackack_acq-multiecho_echo-4_part-mag_desc-preproc_bold.nii.gz\n",
      "Dropping 2 volumes from sub-06_ses-2_task-frackack_acq-multiecho_echo-5_part-mag_desc-preproc_bold.nii.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO     tedana:tedana_workflow:483 Using output directory: /Users/taylor/Documents/datasets/xcpd_example/derivatives/tedana/sub-06/ses-2/func/sub-06_ses-2_task-frackack_acq-multiecho\n",
      "INFO     tedana:tedana_workflow:501 Loading input data: ['/Users/taylor/Documents/datasets/xcpd_example/derivatives/reduced_files/sub-06_ses-2_task-frackack_acq-multiecho_echo-1_part-mag_desc-preproc_bold.nii.gz', '/Users/taylor/Documents/datasets/xcpd_example/derivatives/reduced_files/sub-06_ses-2_task-frackack_acq-multiecho_echo-2_part-mag_desc-preproc_bold.nii.gz', '/Users/taylor/Documents/datasets/xcpd_example/derivatives/reduced_files/sub-06_ses-2_task-frackack_acq-multiecho_echo-3_part-mag_desc-preproc_bold.nii.gz', '/Users/taylor/Documents/datasets/xcpd_example/derivatives/reduced_files/sub-06_ses-2_task-frackack_acq-multiecho_echo-4_part-mag_desc-preproc_bold.nii.gz', '/Users/taylor/Documents/datasets/xcpd_example/derivatives/reduced_files/sub-06_ses-2_task-frackack_acq-multiecho_echo-5_part-mag_desc-preproc_bold.nii.gz']\n",
      "INFO     io:__init__:155 Generating figures directory: /Users/taylor/Documents/datasets/xcpd_example/derivatives/tedana/sub-06/ses-2/func/sub-06_ses-2_task-frackack_acq-multiecho/figures\n",
      "INFO     tedana:tedana_workflow:564 Using user-defined mask\n",
      "INFO     tedana:tedana_workflow:612 Computing T2* map\n",
      "INFO     combine:make_optcom:192 Optimally combining data with voxel-wise T2* estimates\n",
      "INFO     tedana:tedana_workflow:637 Writing optimally combined data set: /Users/taylor/Documents/datasets/xcpd_example/derivatives/tedana/sub-06/ses-2/func/sub-06_ses-2_task-frackack_acq-multiecho/sub-06_ses-2_task-frackack_acq-multiecho_desc-optcom_bold.nii.gz\n",
      "INFO     pca:tedpca:203 Computing PCA of optimally combined multi-echo data with selection criteria: aic\n",
      "/Users/taylor/Documents/tsalo/tedana/tedana/io.py:800: UserWarning: Data array used to create a new image contains 64-bit ints. This is likely due to creating the array with numpy and passing `int` as the `dtype`. Many tools such as FSL and SPM cannot deal with int64 in Nifti images, so for compatibility the data has been converted to int32.\n",
      "  nii = new_img_like(ref_img, newdata, affine=affine, copy_header=copy_header)\n",
      "/opt/miniconda3/envs/salo/lib/python3.9/site-packages/sklearn/utils/deprecation.py:101: FutureWarning: Attribute `n_features_` was deprecated in version 1.2 and will be removed in 1.4. Use `n_features_in_` instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "INFO     pca:tedpca:243 Optimal number of components based on different criteria:\n",
      "INFO     pca:tedpca:244 AIC: 43 | KIC: 37 | MDL: 26 | 90% varexp: 189 | 95% varexp: 211\n",
      "INFO     pca:tedpca:249 Explained variance based on different criteria:\n",
      "INFO     pca:tedpca:250 AIC: 0.533% | KIC: 0.514% | MDL: 0.474% | 90% varexp: 0.901% | 95% varexp: 0.951%\n",
      "INFO     pca:tedpca:267 Plotting maPCA optimization curves\n",
      "INFO     collect:generate_metrics:121 Calculating weight maps\n",
      "INFO     collect:generate_metrics:130 Calculating parameter estimate maps for optimally combined data\n",
      "INFO     collect:generate_metrics:143 Calculating z-statistic maps\n",
      "INFO     collect:generate_metrics:153 Calculating F-statistic maps\n",
      "INFO     collect:generate_metrics:173 Thresholding z-statistic maps\n",
      "INFO     collect:generate_metrics:180 Calculating T2* F-statistic maps\n",
      "INFO     collect:generate_metrics:187 Calculating S0 F-statistic maps\n",
      "INFO     collect:generate_metrics:195 Counting significant voxels in T2* F-statistic maps\n",
      "INFO     collect:generate_metrics:201 Counting significant voxels in S0 F-statistic maps\n",
      "INFO     collect:generate_metrics:208 Thresholding optimal combination beta maps to match T2* F-statistic maps\n",
      "INFO     collect:generate_metrics:214 Thresholding optimal combination beta maps to match S0 F-statistic maps\n",
      "INFO     collect:generate_metrics:221 Calculating kappa and rho\n",
      "INFO     collect:generate_metrics:230 Calculating variance explained\n",
      "INFO     collect:generate_metrics:236 Calculating normalized variance explained\n",
      "INFO     collect:generate_metrics:243 Calculating DSI between thresholded T2* F-statistic and optimal combination beta maps\n",
      "INFO     collect:generate_metrics:254 Calculating DSI between thresholded S0 F-statistic and optimal combination beta maps\n",
      "INFO     collect:generate_metrics:265 Calculating signal-noise t-statistics\n",
      "INFO     collect:generate_metrics:303 Counting significant noise voxels from z-statistic maps\n",
      "INFO     collect:generate_metrics:314 Calculating decision table score\n",
      "INFO     pca:tedpca:397 Selected 43 components with 53.33% normalized variance explained using aic dimensionality estimate\n",
      "/Users/taylor/Documents/tsalo/tedana/tedana/io.py:346: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  deblanked = data.replace(\"\", np.nan)\n",
      "INFO     ica:tedica:80 ICA with random seed 42 converged in 84 iterations\n",
      "INFO     tedana:tedana_workflow:671 Making second component selection guess from ICA results\n",
      "INFO     collect:generate_metrics:121 Calculating weight maps\n",
      "INFO     collect:generate_metrics:130 Calculating parameter estimate maps for optimally combined data\n",
      "INFO     collect:generate_metrics:143 Calculating z-statistic maps\n",
      "INFO     collect:generate_metrics:153 Calculating F-statistic maps\n",
      "INFO     collect:generate_metrics:173 Thresholding z-statistic maps\n",
      "INFO     collect:generate_metrics:180 Calculating T2* F-statistic maps\n",
      "INFO     collect:generate_metrics:187 Calculating S0 F-statistic maps\n",
      "INFO     collect:generate_metrics:195 Counting significant voxels in T2* F-statistic maps\n",
      "INFO     collect:generate_metrics:201 Counting significant voxels in S0 F-statistic maps\n",
      "INFO     collect:generate_metrics:208 Thresholding optimal combination beta maps to match T2* F-statistic maps\n",
      "INFO     collect:generate_metrics:214 Thresholding optimal combination beta maps to match S0 F-statistic maps\n",
      "INFO     collect:generate_metrics:221 Calculating kappa and rho\n",
      "INFO     collect:generate_metrics:230 Calculating variance explained\n",
      "INFO     collect:generate_metrics:236 Calculating normalized variance explained\n",
      "INFO     collect:generate_metrics:243 Calculating DSI between thresholded T2* F-statistic and optimal combination beta maps\n",
      "INFO     collect:generate_metrics:254 Calculating DSI between thresholded S0 F-statistic and optimal combination beta maps\n",
      "INFO     collect:generate_metrics:265 Calculating signal-noise t-statistics\n",
      "INFO     collect:generate_metrics:303 Counting significant noise voxels from z-statistic maps\n",
      "INFO     collect:generate_metrics:314 Calculating decision table score\n",
      "INFO     tedica:automatic_selection:58 Performing ICA component selection with Kundu decision tree v2.5\n",
      "WARNING  component_selector:validate_tree:120 Decision tree includes fields that are not used or logged {'_comment'}\n",
      "INFO     component_selector:__init__:292 Performing component selection with kundu_MEICA27_decision_tree\n",
      "INFO     component_selector:__init__:293 Following the decision tree close to one designed by Prantik Kundu\n",
      "INFO     selection_nodes:manual_classify:104 Step 0: manual_classify: Set all to unclassified \n",
      "INFO     selection_utils:log_decision_tree_step:421 Step 0: manual_classify applied to 43 components. 43 True -> unclassified. 0 False -> nochange.\n",
      "INFO     selection_nodes:manual_classify:136 Step 0: manual_classify component classification tags are cleared\n",
      "INFO     selection_utils:log_classification_counts:466 Step 0: Total component classifications: 43 unclassified\n",
      "INFO     selection_nodes:dec_left_op_right:389 Step 1: left_op_right: rejected if rho>kappa, else nochange\n",
      "INFO     selection_utils:log_decision_tree_step:421 Step 1: left_op_right applied to 43 components. 23 True -> rejected. 20 False -> nochange.\n",
      "INFO     selection_utils:log_classification_counts:466 Step 1: Total component classifications: 23 rejected, 20 unclassified\n",
      "INFO     selection_nodes:dec_left_op_right:389 Step 2: left_op_right: rejected if ['countsigFS0>countsigFT2 & countsigFT2>0'], else nochange\n",
      "INFO     selection_utils:log_decision_tree_step:421 Step 2: left_op_right applied to 43 components. 22 True -> rejected. 21 False -> nochange.\n",
      "INFO     selection_utils:log_classification_counts:466 Step 2: Total component classifications: 23 rejected, 20 unclassified\n",
      "INFO     selection_nodes:calc_median:653 Step 3: calc_median: Median(median_varex)\n",
      "INFO     selection_utils:log_decision_tree_step:433 Step 3: calc_median calculated: median_varex=0.8915732230861336\n",
      "INFO     selection_utils:log_classification_counts:466 Step 3: Total component classifications: 23 rejected, 20 unclassified\n",
      "INFO     selection_nodes:dec_left_op_right:389 Step 4: left_op_right: rejected if ['dice_FS0>dice_FT2 & variance explained>0.89'], else nochange\n",
      "INFO     selection_utils:log_decision_tree_step:421 Step 4: left_op_right applied to 43 components. 1 True -> rejected. 42 False -> nochange.\n",
      "INFO     selection_utils:log_classification_counts:466 Step 4: Total component classifications: 23 rejected, 20 unclassified\n",
      "INFO     selection_nodes:dec_left_op_right:389 Step 5: left_op_right: rejected if ['0>signal-noise_t & variance explained>0.89'], else nochange\n",
      "INFO     selection_utils:log_decision_tree_step:421 Step 5: left_op_right applied to 43 components. 8 True -> rejected. 35 False -> nochange.\n",
      "INFO     selection_utils:log_classification_counts:466 Step 5: Total component classifications: 29 rejected, 14 unclassified\n",
      "INFO     selection_nodes:calc_kappa_elbow:757 Step 6: calc_kappa_elbow: Calc Kappa Elbow\n",
      "INFO     selection_utils:kappa_elbow_kundu:636 Calculating kappa elbow based on min of all and nonsig components.\n",
      "INFO     selection_utils:log_decision_tree_step:433 Step 6: calc_kappa_elbow calculated: kappa_elbow_kundu=10.438805709129028, kappa_allcomps_elbow=23.164006424290147, kappa_nonsig_elbow=10.438805709129028, varex_upper_p=1.5540679811742013\n",
      "INFO     selection_utils:log_classification_counts:466 Step 6: Total component classifications: 29 rejected, 14 unclassified\n",
      "INFO     selection_nodes:dec_reclassify_high_var_comps:1116 Step 7: reclassify_high_var_comps: Change unclassified to unclass_highvar for the top couple of components with the highest jumps in variance\n",
      "INFO     selection_utils:comptable_classification_changer:279 Step 7: No components fit criterion True to change classification\n",
      "INFO     selection_utils:log_decision_tree_step:421 Step 7: reclassify_high_var_comps applied to 14 components. 0 True -> unclass_highvar. 14 False -> nochange.\n",
      "INFO     selection_utils:log_classification_counts:466 Step 7: Total component classifications: 29 rejected, 14 unclassified\n",
      "INFO     selection_nodes:calc_rho_elbow:878 Step 8: calc_rho_elbow: Calc Rho Elbow\n",
      "INFO     selection_utils:log_decision_tree_step:433 Step 8: calc_rho_elbow calculated: rho_elbow_kundu=13.370053948761191, rho_allcomps_elbow=20.369335091525514, rho_unclassified_elbow=12.032179332581272, elbow_f05=7.708647422176786\n",
      "INFO     selection_utils:log_classification_counts:466 Step 8: Total component classifications: 29 rejected, 14 unclassified\n",
      "INFO     selection_nodes:dec_left_op_right:389 Step 9: left_op_right: provisionalaccept if kappa>=10.44, else nochange\n",
      "INFO     selection_utils:log_decision_tree_step:421 Step 9: left_op_right applied to 14 components. 14 True -> provisionalaccept. 0 False -> nochange.\n",
      "INFO     selection_utils:log_classification_counts:466 Step 9: Total component classifications: 14 provisionalaccept, 29 rejected\n",
      "INFO     selection_nodes:dec_left_op_right:389 Step 10: left_op_right: unclassified if rho>13.37, else nochange\n",
      "INFO     selection_utils:log_decision_tree_step:421 Step 10: left_op_right applied to 14 components. 3 True -> unclassified. 11 False -> nochange.\n",
      "INFO     selection_utils:log_classification_counts:466 Step 10: Total component classifications: 11 provisionalaccept, 29 rejected, 3 unclassified\n",
      "INFO     selection_nodes:dec_classification_doesnt_exist:1005 Step 11: classification_doesnt_exist: Change ['provisionalaccept', 'unclassified', 'unclass_highvar'] to accepted if less than 2 components with provisionalaccept exist\n",
      "INFO     selection_nodes:dec_classification_doesnt_exist:1007 Step 11: classification_doesnt_exist If nothing is provisionally accepted by this point, then rerun ICA & selection. If max iterations of rerunning done, then accept everything not already rejected\n",
      "INFO     selection_utils:log_decision_tree_step:421 Step 11: classification_doesnt_exist applied to 14 components. None True -> 0. None False -> 14.\n",
      "INFO     selection_utils:log_classification_counts:466 Step 11: Total component classifications: 11 provisionalaccept, 29 rejected, 3 unclassified\n",
      "INFO     selection_nodes:calc_varex_thresh:1304 Step 12: calc_varex_thresh: Calc varex_upper_thresh, 90th percentile threshold\n",
      "INFO     selection_utils:log_decision_tree_step:433 Step 12: calc_varex_thresh calculated: varex_upper_thresh=1.2513231953438055, upper_perc=90\n",
      "INFO     selection_utils:log_classification_counts:466 Step 12: Total component classifications: 11 provisionalaccept, 29 rejected, 3 unclassified\n",
      "INFO     selection_nodes:calc_varex_thresh:1304 Step 13: calc_varex_thresh: Calc varex_lower_thresh, 25th percentile threshold\n",
      "INFO     selection_utils:log_decision_tree_step:433 Step 13: calc_varex_thresh calculated: varex_lower_thresh=0.6120339153926211, lower_perc=25\n",
      "INFO     selection_utils:log_classification_counts:466 Step 13: Total component classifications: 11 provisionalaccept, 29 rejected, 3 unclassified\n",
      "INFO     selection_utils:get_extend_factor:813 extend_factor=2.0, based on number of fMRI volumes\n",
      "INFO     selection_utils:log_decision_tree_step:433 Step 14: calc_extend_factor calculated: extend_factor=2.0\n",
      "INFO     selection_utils:log_classification_counts:466 Step 14: Total component classifications: 11 provisionalaccept, 29 rejected, 3 unclassified\n",
      "INFO     selection_utils:log_decision_tree_step:433 Step 15: calc_max_good_meanmetricrank calculated: max_good_meanmetricrank=22.0\n",
      "INFO     selection_utils:log_classification_counts:466 Step 15: Total component classifications: 11 provisionalaccept, 29 rejected, 3 unclassified\n",
      "INFO     selection_utils:log_decision_tree_step:433 Step 16: calc_varex_kappa_ratio calculated: kappa_rate=33.48806850668938\n",
      "INFO     selection_utils:log_classification_counts:466 Step 16: Total component classifications: 11 provisionalaccept, 29 rejected, 3 unclassified\n",
      "INFO     selection_nodes:dec_left_op_right:389 Step 17: left_op_right: rejected if ['d_table_score>22.0 & variance explained>2.0*1.25'], else nochange\n",
      "INFO     selection_nodes:dec_left_op_right:391 Step 17: left_op_right If variance and d_table_scores are high, then reject\n",
      "INFO     selection_utils:comptable_classification_changer:279 Step 17: No components fit criterion True to change classification\n",
      "INFO     selection_utils:log_decision_tree_step:421 Step 17: left_op_right applied to 14 components. 0 True -> rejected. 14 False -> nochange.\n",
      "INFO     selection_utils:log_classification_counts:466 Step 17: Total component classifications: 11 provisionalaccept, 29 rejected, 3 unclassified\n",
      "INFO     selection_nodes:dec_left_op_right:389 Step 18: left_op_right: accepted if ['d_table_score>22.0 & variance explained<=0.61 & kappa<=10.44'], else nochange\n",
      "INFO     selection_nodes:dec_left_op_right:391 Step 18: left_op_right If low variance, accept even if bad kappa & d_table_scores\n",
      "INFO     selection_utils:comptable_classification_changer:279 Step 18: No components fit criterion True to change classification\n",
      "INFO     selection_utils:log_decision_tree_step:421 Step 18: left_op_right applied to 14 components. 0 True -> accepted. 14 False -> nochange.\n",
      "INFO     selection_utils:log_classification_counts:466 Step 18: Total component classifications: 11 provisionalaccept, 29 rejected, 3 unclassified\n",
      "INFO     selection_nodes:dec_classification_doesnt_exist:1005 Step 19: classification_doesnt_exist: Change ['provisionalaccept', 'unclassified', 'unclass_highvar'] to accepted if ['unclassified', 'unclass_highvar'] doesn't exist\n",
      "INFO     selection_nodes:dec_classification_doesnt_exist:1007 Step 19: classification_doesnt_exist If nothing left is unclassified, then accept all\n",
      "INFO     selection_utils:log_decision_tree_step:421 Step 19: classification_doesnt_exist applied to 14 components. None True -> 0. None False -> 14.\n",
      "INFO     selection_utils:log_classification_counts:466 Step 19: Total component classifications: 11 provisionalaccept, 29 rejected, 3 unclassified\n",
      "INFO     selection_nodes:calc_revised_meanmetricrank_guesses:1762 Step 20: calc_revised_meanmetricrank_guesses: Calc revised d_table_score & num accepted component guesses\n",
      "INFO     selection_utils:log_decision_tree_step:433 Step 20: calc_revised_meanmetricrank_guesses calculated: num_acc_guess=12, conservative_guess=6.0, restrict_factor=2\n",
      "INFO     selection_utils:log_classification_counts:466 Step 20: Total component classifications: 11 provisionalaccept, 29 rejected, 3 unclassified\n",
      "INFO     selection_nodes:dec_left_op_right:389 Step 21: left_op_right: rejected if ['d_table_score_node20>6.0 & varex kappa ratio>2*2.0 & variance explained>2.0*1.25'], else nochange\n",
      "INFO     selection_nodes:dec_left_op_right:391 Step 21: left_op_right Reject if a combination of kappa, variance, and other factors are ranked worse than others\n",
      "INFO     selection_utils:comptable_classification_changer:279 Step 21: No components fit criterion True to change classification\n",
      "INFO     selection_utils:log_decision_tree_step:421 Step 21: left_op_right applied to 14 components. 0 True -> rejected. 14 False -> nochange.\n",
      "INFO     selection_utils:log_classification_counts:466 Step 21: Total component classifications: 11 provisionalaccept, 29 rejected, 3 unclassified\n",
      "INFO     selection_nodes:dec_left_op_right:389 Step 22: left_op_right: rejected if ['d_table_score_node20>0.9*12 & variance explained>2.0*0.61'], else nochange\n",
      "INFO     selection_nodes:dec_left_op_right:391 Step 22: left_op_right Reject if a combination of variance and ranks of other metrics are worse than others\n",
      "INFO     selection_utils:comptable_classification_changer:279 Step 22: No components fit criterion True to change classification\n",
      "INFO     selection_utils:log_decision_tree_step:421 Step 22: left_op_right applied to 14 components. 0 True -> rejected. 14 False -> nochange.\n",
      "INFO     selection_utils:log_classification_counts:466 Step 22: Total component classifications: 11 provisionalaccept, 29 rejected, 3 unclassified\n",
      "INFO     selection_nodes:calc_varex_thresh:1304 Step 23: calc_varex_thresh: Calc varex_new_lower_thresh, 25th percentile threshold\n",
      "INFO     selection_utils:log_decision_tree_step:433 Step 23: calc_varex_thresh calculated: varex_new_lower_thresh=0.8542315797048141, new_lower_perc=25\n",
      "INFO     selection_utils:log_classification_counts:466 Step 23: Total component classifications: 11 provisionalaccept, 29 rejected, 3 unclassified\n",
      "INFO     selection_nodes:dec_left_op_right:389 Step 24: left_op_right: accepted if ['d_table_score_node20>12 & variance explained>0.85'], else nochange\n",
      "INFO     selection_nodes:dec_left_op_right:391 Step 24: left_op_right Accept components with a bad d_table_score, but are at the higher end of the remaining variance so more cautious to not remove\n",
      "INFO     selection_utils:comptable_classification_changer:279 Step 24: No components fit criterion True to change classification\n",
      "INFO     selection_utils:log_decision_tree_step:421 Step 24: left_op_right applied to 14 components. 0 True -> accepted. 14 False -> nochange.\n",
      "INFO     selection_utils:log_classification_counts:466 Step 24: Total component classifications: 11 provisionalaccept, 29 rejected, 3 unclassified\n",
      "INFO     selection_nodes:dec_left_op_right:389 Step 25: left_op_right: accepted if ['kappa<=10.44 & variance explained>0.85'], else nochange\n",
      "INFO     selection_nodes:dec_left_op_right:391 Step 25: left_op_right For not already rejected components, accept ones below the kappa elbow, but at the higher end of the remaining variance so more cautious to not remove\n",
      "INFO     selection_utils:comptable_classification_changer:279 Step 25: No components fit criterion True to change classification\n",
      "INFO     selection_utils:log_decision_tree_step:421 Step 25: left_op_right applied to 14 components. 0 True -> accepted. 14 False -> nochange.\n",
      "INFO     selection_utils:log_classification_counts:466 Step 25: Total component classifications: 11 provisionalaccept, 29 rejected, 3 unclassified\n",
      "INFO     selection_nodes:manual_classify:104 Step 26: manual_classify: Set ['provisionalaccept', 'unclassified', 'unclass_highvar'] to accepted \n",
      "INFO     selection_nodes:manual_classify:106 Step 26: manual_classify Anything still provisional (accepted or rejected) should be accepted\n",
      "INFO     selection_utils:log_decision_tree_step:421 Step 26: manual_classify applied to 14 components. 14 True -> accepted. 0 False -> nochange.\n",
      "INFO     selection_utils:log_classification_counts:466 Step 26: Total component classifications: 14 accepted, 29 rejected\n",
      "WARNING  component_selector:are_only_necessary_metrics_used:473 Decision tree kundu used the following metrics that were not declared as necessary: {'d_table_score_node20', 'varex kappa ratio'}\n",
      "INFO     io:denoise_ts:527 Variance explained by decomposition: 66.66%\n",
      "INFO     io:write_split_ts:614 Writing denoised time series: /Users/taylor/Documents/datasets/xcpd_example/derivatives/tedana/sub-06/ses-2/func/sub-06_ses-2_task-frackack_acq-multiecho/sub-06_ses-2_task-frackack_acq-multiecho_desc-denoised_bold.nii.gz\n",
      "INFO     io:writeresults:663 Writing full ICA coefficient feature set: /Users/taylor/Documents/datasets/xcpd_example/derivatives/tedana/sub-06/ses-2/func/sub-06_ses-2_task-frackack_acq-multiecho/sub-06_ses-2_task-frackack_acq-multiecho_desc-ICA_components.nii.gz\n",
      "INFO     io:writeresults:667 Writing denoised ICA coefficient feature set: /Users/taylor/Documents/datasets/xcpd_example/derivatives/tedana/sub-06/ses-2/func/sub-06_ses-2_task-frackack_acq-multiecho/sub-06_ses-2_task-frackack_acq-multiecho_desc-ICAAccepted_components.nii.gz\n",
      "INFO     io:writeresults:673 Writing Z-normalized spatial component maps: /Users/taylor/Documents/datasets/xcpd_example/derivatives/tedana/sub-06/ses-2/func/sub-06_ses-2_task-frackack_acq-multiecho/sub-06_ses-2_task-frackack_acq-multiecho_desc-ICAAccepted_stat-z_components.nii.gz\n",
      "INFO     tedana:tedana_workflow:884 Making figures folder with static component maps and timecourse plots.\n",
      "INFO     io:denoise_ts:527 Variance explained by decomposition: 66.66%\n",
      "/Users/taylor/Documents/tsalo/tedana/tedana/io.py:800: UserWarning: Data array used to create a new image contains 64-bit ints. This is likely due to creating the array with numpy and passing `int` as the `dtype`. Many tools such as FSL and SPM cannot deal with int64 in Nifti images, so for compatibility the data has been converted to int32.\n",
      "  nii = new_img_like(ref_img, newdata, affine=affine, copy_header=copy_header)\n",
      "/Users/taylor/Documents/tsalo/tedana/tedana/io.py:800: UserWarning: Data array used to create a new image contains 64-bit ints. This is likely due to creating the array with numpy and passing `int` as the `dtype`. Many tools such as FSL and SPM cannot deal with int64 in Nifti images, so for compatibility the data has been converted to int32.\n",
      "  nii = new_img_like(ref_img, newdata, affine=affine, copy_header=copy_header)\n",
      "INFO     tedana:tedana_workflow:907 Generating dynamic report\n",
      "INFO     tedana:tedana_workflow:910 Workflow completed\n"
     ]
    }
   ],
   "source": [
    "dset_dir = \"/Users/taylor/Documents/datasets/xcpd_example\"\n",
    "deriv_dir = os.path.join(dset_dir, \"derivatives\")\n",
    "func_dir = os.path.join(deriv_dir, \"fmriprep/sub-06/ses-2/func\")\n",
    "\n",
    "ECHO_TIMES = [14.2, 38.93, 63.66, 88.39, 113.12]  # hardcoded bc i'm lazy\n",
    "prefix = f\"sub-06_ses-2_task-frackack_acq-multiecho\"\n",
    "bold_files = [\n",
    "    os.path.join(func_dir,f\"{prefix}_echo-{echo + 1}_part-mag_desc-preproc_bold.nii.gz\")\n",
    "    for echo in range(len(ECHO_TIMES))\n",
    "]\n",
    "confounds_file = os.path.join(func_dir,  f\"{prefix}_part-mag_desc-confounds_timeseries.tsv\")\n",
    "mask_file = os.path.join(func_dir, f\"{prefix}_part-mag_desc-brain_mask.nii.gz\")\n",
    "\n",
    "# Write tedana outputs to BIDS-like structure,\n",
    "# but use a separate folder for each run.\n",
    "tedana_out_dir = os.path.join(deriv_dir, \"tedana/sub-06/ses-2/func\", prefix)\n",
    "os.makedirs(tedana_out_dir, exist_ok=True)\n",
    "# A folder for all of the shortened files.\n",
    "tedana_temp_dir = os.path.join(dset_dir, \"derivatives\", \"reduced_files\")\n",
    "os.makedirs(tedana_temp_dir, exist_ok=True)\n",
    "\n",
    "shortened_files, n_dummy_vols = drop_dummy_vols(\n",
    "    bold_files=bold_files,\n",
    "    confounds_file=confounds_file,\n",
    "    temp_dir=tedana_temp_dir,\n",
    ")\n",
    "\n",
    "tedana_workflow(\n",
    "    data=shortened_files,\n",
    "    tes=ECHO_TIMES,\n",
    "    out_dir=tedana_out_dir,\n",
    "    mask=mask_file,\n",
    "    prefix=prefix,\n",
    "    fittype=\"loglin\",\n",
    "    tedort=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Label tedana components and fill in dummy volumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load (orthogonalized) mixing matrix and classifications from tedana\n",
    "mixing_matrix = os.path.join(tedana_out_dir, f\"{prefix}_desc-ICAOrth_mixing.tsv\")\n",
    "metrics_df = os.path.join(tedana_out_dir, f\"{prefix}_desc-tedana_metrics.tsv\")\n",
    "mixing_matrix = pd.read_table(mixing_matrix)\n",
    "metrics_df = pd.read_table(metrics_df)\n",
    "\n",
    "# Prepend \"signal__\" to all accepted components' column names\n",
    "rejected_columns = metrics_df.loc[metrics_df[\"classification\"] == \"rejected\", \"Component\"]\n",
    "mixing_matrix = mixing_matrix[rejected_columns]\n",
    "\n",
    "# Add dummyvols back in to beginning of matrix, as copies of the first row\n",
    "mixing_matrix_data = mixing_matrix.to_numpy()\n",
    "first_row = mixing_matrix_data[0, :]\n",
    "leading_rows = np.ones((n_dummy_vols, mixing_matrix.shape[1])) * first_row\n",
    "new_mixing_matrix_arr = np.vstack((leading_rows, mixing_matrix_data))\n",
    "new_mixing_matrix = pd.DataFrame(new_mixing_matrix_arr, columns=mixing_matrix.columns)\n",
    "\n",
    "# Write out to custom confounds folder\n",
    "custom_confounds_folder = os.path.join(deriv_dir, \"custom_confounds_for_xcpd\")\n",
    "os.makedirs(custom_confounds_folder, exist_ok=True)\n",
    "# use the same name as the fMRIPrep confounds, but in the new folder\n",
    "custom_confounds_file = os.path.join(custom_confounds_folder, os.path.basename(confounds_file))\n",
    "new_mixing_matrix.to_csv(custom_confounds_file, sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Run XCP-D with tedana-derived custom confounds\n",
    "\n",
    "Note: dummy-scans must match between tedana and XCP-D\n",
    "```bash\n",
    "docker run --rm -u $(id -u) \\\n",
    "    -v /Users/taylor/Documents/datasets/ds003643:/bids-input:rw \\\n",
    "    -v /Users/taylor/Documents/tsalo/xcp_d/xcp_d:/usr/local/miniconda/lib/python3.8/site-packages/xcp_d \\\n",
    "    -v /Users/taylor/Documents/tsalo/xcp_d_testing/data/license.txt:/license.txt --env FS_LICENSE=/license.txt \\\n",
    "    pennlinc/xcp_d:unstable \\\n",
    "    /bids-input/derivatives/fmriprep \\\n",
    "    /bids-input/derivatives \\\n",
    "    participant \\\n",
    "    -w /bids-input/derivatives/work \\\n",
    "    --participant_label EN100 \\\n",
    "    --nuisance-regressors 27P \\\n",
    "    --custom_confounds /bids-input/derivatives/custom_confounds_for_xcpd \\\n",
    "    --dummy-scans auto \\\n",
    "    --bids-filter-file /bids-input/derivatives/code/filter_file.json \\\n",
    "    -vvv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
