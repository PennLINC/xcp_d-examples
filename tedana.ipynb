{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fMRIPrep + tedana + XCPD\n",
    "\n",
    "There are three key elements to running tedana + XCPD:\n",
    "\n",
    "1.  XCPD is very strict about how it expects external (i.e., custom) confounds files to be organized and named.\n",
    "    We need to copy the associated tedana derivatives (i.e., the mixing matrices) to a new folder and rename them,\n",
    "    so that XCPD can find them.\n",
    "2.  Tedana's ICA will probably look bad if dummy scans haven't been dropped from the BOLD file.\n",
    "    See [this discussion](https://github.com/ME-ICA/tedana/discussions/899).\n",
    "    Therefore, we must drop the dummy scans before running tedana, \n",
    "    then buffer the mixing matrix produced by tedana with dummy data to fill in those scans,\n",
    "    and finally run XCPD with the same number of dummy scans flagged, \n",
    "    so those volumes will ultimately be ignored.\n",
    "3.  Simply regressing out \"bad\" components flagged by tedana is a bad idea.\n",
    "    This is known as \"aggressive\" denoising, and is not recommended.\n",
    "    Instead, we must include the \"good\" components in the regression as well,\n",
    "    but then reconstruct the noise signal using only the nuisance regressors \n",
    "    (\"bad\" components and any other regressors we want, like motion parameters)\n",
    "    and their associated parameter estimates from the regression.\n",
    "    This is known as \"non-aggressive\" denoising.\n",
    "    For more information on this, \n",
    "    see [tedana's documentation](https://tedana.readthedocs.io/en/latest/denoising.html).\n",
    "    XCPD will support this by flagging any confounds starting with `signal__`\n",
    "    as signal regressors to use for a non-aggressive denoising strategy.\n",
    "    This is implemented in [PennLINC/xcp_d#697](https://github.com/PennLINC/xcp_d/pull/697)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Run fMRIPrep 22.0.0+ with `--me-output-echos` flag\n",
    "\n",
    "This has already been handled for this dataset. The data are available at https://gin.g-node.org/ME-ICA/ds003643-fmriprep-derivatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Remove dummy scans from fMRIPrep files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tedana.workflows import tedana_workflow\n",
    "\n",
    "\n",
    "def _flag_dummyvols(confounds_file):\n",
    "    \"\"\"Identify the number of dummy volumes flagged in an fMRIPrep confounds file.\"\"\"\n",
    "    confounds_df = pd.read_table(confounds_file)\n",
    "    nss_cols = [c for c in confounds_df.columns if c.startswith(\"non_steady_state_outlier\")]\n",
    "    if nss_cols:\n",
    "        initial_volumes_df = confounds_df[nss_cols]\n",
    "        dummy_vols = np.any(initial_volumes_df.to_numpy(), axis=1)\n",
    "        dummy_vols = np.where(dummy_vols)[0]\n",
    "\n",
    "        # reasonably assumes all NSS volumes are contiguous\n",
    "        n_dummy_vols = int(dummy_vols[-1] + 1)\n",
    "        # dummy_scans = 10\n",
    "    else:\n",
    "        n_dummy_vols = 0\n",
    "    \n",
    "    return n_dummy_vols\n",
    "\n",
    "\n",
    "def _remove_dummyvols(in_file, out_file, n_dummy_vols):\n",
    "    \"\"\"Remove dummy volumes from in_file and write out to out_file.\n",
    "    \n",
    "    If n_dummy_vols is 0, then just return out_file.\n",
    "    \"\"\"\n",
    "    if n_dummy_vols:\n",
    "        print(f\"Dropping {n_dummy_vols} volumes from {os.path.basename(in_file)}\")\n",
    "\n",
    "        img = nib.load(in_file)\n",
    "        img = img.slicer[..., n_dummy_vols:]\n",
    "        img.to_filename(out_file)\n",
    "    else:\n",
    "        out_file = in_file\n",
    "\n",
    "    return out_file\n",
    "\n",
    "\n",
    "def drop_dummy_vols(bold_files, confounds_file, temp_dir=\".\"):\n",
    "    \"\"\"Remove dummy volumes from a list of files.\n",
    "    \n",
    "    This infers the number of dummy volumes from the confounds file.\n",
    "    The shortened files are written out to temp_dir with the same\n",
    "    filenames as the original files.\n",
    "    \"\"\"\n",
    "    shortened_files = []\n",
    "    n_dummy_vols = _flag_dummyvols(confounds_file)\n",
    "    for bold_file in bold_files:\n",
    "        if n_dummy_vols:\n",
    "            temp_file = os.path.join(temp_dir, os.path.basename(bold_file))\n",
    "            shortened_file = _remove_dummyvols(bold_file, temp_file, n_dummy_vols)\n",
    "        else:\n",
    "            shortened_file = bold_file\n",
    "        shortened_files.append(shortened_file)\n",
    "\n",
    "    return shortened_files, n_dummy_vols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping 4 volumes from sub-EN100_task-lppEN_run-1_echo-1_desc-preproc_bold.nii.gz\n",
      "Dropping 4 volumes from sub-EN100_task-lppEN_run-1_echo-2_desc-preproc_bold.nii.gz\n",
      "Dropping 4 volumes from sub-EN100_task-lppEN_run-1_echo-3_desc-preproc_bold.nii.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO     tedana:tedana_workflow:466 Using output directory: /Users/taylor/Documents/datasets/ds003643/derivatives/tedana/sub-EN100/func/sub-EN100_task-lppEN_run-1\n",
      "INFO     tedana:tedana_workflow:479 Loading input data: ['/Users/taylor/Documents/datasets/ds003643/derivatives/reduced_files/sub-EN100_task-lppEN_run-1_echo-1_desc-preproc_bold.nii.gz', '/Users/taylor/Documents/datasets/ds003643/derivatives/reduced_files/sub-EN100_task-lppEN_run-1_echo-2_desc-preproc_bold.nii.gz', '/Users/taylor/Documents/datasets/ds003643/derivatives/reduced_files/sub-EN100_task-lppEN_run-1_echo-3_desc-preproc_bold.nii.gz']\n",
      "INFO     tedana:tedana_workflow:561 Using user-defined mask\n",
      "INFO     tedana:tedana_workflow:609 Computing T2* map\n",
      "INFO     combine:make_optcom:242 Optimally combining data with voxel-wise T2* estimates\n",
      "INFO     tedana:tedana_workflow:634 Writing optimally combined data set: /Users/taylor/Documents/datasets/ds003643/derivatives/tedana/sub-EN100/func/sub-EN100_task-lppEN_run-1/sub-EN100_task-lppEN_run-1_desc-optcom_bold.nii.gz\n",
      "INFO     pca:tedpca:228 Computing PCA of optimally combined multi-echo data with selection criteria: aic\n",
      "/Users/taylor/Documents/tsalo/tedana/tedana/io.py:640: UserWarning: Data array used to create a new image contains 64-bit ints. This is likely due to creating the array with numpy and passing `int` as the `dtype`. Many tools such as FSL and SPM cannot deal with int64 in Nifti images, so for compatibility the data has been converted to int32.\n",
      "  nii = new_img_like(ref_img, newdata, affine=affine, copy_header=copy_header)\n",
      "INFO     collect:generate_metrics:123 Calculating weight maps\n",
      "INFO     collect:generate_metrics:132 Calculating parameter estimate maps for optimally combined data\n",
      "INFO     collect:generate_metrics:145 Calculating z-statistic maps\n",
      "INFO     collect:generate_metrics:155 Calculating F-statistic maps\n",
      "INFO     collect:generate_metrics:165 Thresholding z-statistic maps\n",
      "INFO     collect:generate_metrics:172 Calculating T2* F-statistic maps\n",
      "INFO     collect:generate_metrics:179 Calculating S0 F-statistic maps\n",
      "INFO     collect:generate_metrics:187 Counting significant voxels in T2* F-statistic maps\n",
      "INFO     collect:generate_metrics:193 Counting significant voxels in S0 F-statistic maps\n",
      "INFO     collect:generate_metrics:200 Thresholding optimal combination beta maps to match T2* F-statistic maps\n",
      "INFO     collect:generate_metrics:206 Thresholding optimal combination beta maps to match S0 F-statistic maps\n",
      "INFO     collect:generate_metrics:213 Calculating kappa and rho\n",
      "INFO     collect:generate_metrics:222 Calculating variance explained\n",
      "INFO     collect:generate_metrics:228 Calculating normalized variance explained\n",
      "INFO     collect:generate_metrics:235 Calculating DSI between thresholded T2* F-statistic and optimal combination beta maps\n",
      "INFO     collect:generate_metrics:246 Calculating DSI between thresholded S0 F-statistic and optimal combination beta maps\n",
      "INFO     collect:generate_metrics:257 Calculating signal-noise t-statistics\n",
      "INFO     collect:generate_metrics:295 Counting significant noise voxels from z-statistic maps\n",
      "INFO     collect:generate_metrics:306 Calculating decision table score\n",
      "INFO     pca:tedpca:315 Selected 78 components with aic dimensionality detection\n",
      "INFO     ica:tedica:83 ICA with random seed 42 converged in 113 iterations\n",
      "INFO     tedana:tedana_workflow:671 Making second component selection guess from ICA results\n",
      "INFO     collect:generate_metrics:123 Calculating weight maps\n",
      "INFO     collect:generate_metrics:132 Calculating parameter estimate maps for optimally combined data\n",
      "INFO     collect:generate_metrics:145 Calculating z-statistic maps\n",
      "INFO     collect:generate_metrics:155 Calculating F-statistic maps\n",
      "INFO     collect:generate_metrics:165 Thresholding z-statistic maps\n",
      "INFO     collect:generate_metrics:172 Calculating T2* F-statistic maps\n",
      "INFO     collect:generate_metrics:179 Calculating S0 F-statistic maps\n",
      "INFO     collect:generate_metrics:187 Counting significant voxels in T2* F-statistic maps\n",
      "INFO     collect:generate_metrics:193 Counting significant voxels in S0 F-statistic maps\n",
      "INFO     collect:generate_metrics:200 Thresholding optimal combination beta maps to match T2* F-statistic maps\n",
      "INFO     collect:generate_metrics:206 Thresholding optimal combination beta maps to match S0 F-statistic maps\n",
      "INFO     collect:generate_metrics:213 Calculating kappa and rho\n",
      "INFO     collect:generate_metrics:222 Calculating variance explained\n",
      "INFO     collect:generate_metrics:228 Calculating normalized variance explained\n",
      "INFO     collect:generate_metrics:235 Calculating DSI between thresholded T2* F-statistic and optimal combination beta maps\n",
      "INFO     collect:generate_metrics:246 Calculating DSI between thresholded S0 F-statistic and optimal combination beta maps\n",
      "INFO     collect:generate_metrics:257 Calculating signal-noise t-statistics\n",
      "INFO     collect:generate_metrics:295 Counting significant noise voxels from z-statistic maps\n",
      "INFO     collect:generate_metrics:306 Calculating decision table score\n",
      "INFO     tedica:kundu_selection_v2:138 Performing ICA component selection with Kundu decision tree v2.5\n",
      "INFO     io:denoise_ts:374 Variance explained by decomposition: 95.75%\n",
      "INFO     io:write_split_ts:432 Writing high-Kappa time series: /Users/taylor/Documents/datasets/ds003643/derivatives/tedana/sub-EN100/func/sub-EN100_task-lppEN_run-1/sub-EN100_task-lppEN_run-1_desc-optcomAccepted_bold.nii.gz\n",
      "INFO     io:write_split_ts:439 Writing low-Kappa time series: /Users/taylor/Documents/datasets/ds003643/derivatives/tedana/sub-EN100/func/sub-EN100_task-lppEN_run-1/sub-EN100_task-lppEN_run-1_desc-optcomRejected_bold.nii.gz\n",
      "INFO     io:write_split_ts:446 Writing denoised time series: /Users/taylor/Documents/datasets/ds003643/derivatives/tedana/sub-EN100/func/sub-EN100_task-lppEN_run-1/sub-EN100_task-lppEN_run-1_desc-optcomDenoised_bold.nii.gz\n",
      "INFO     io:writeresults:498 Writing full ICA coefficient feature set: /Users/taylor/Documents/datasets/ds003643/derivatives/tedana/sub-EN100/func/sub-EN100_task-lppEN_run-1/sub-EN100_task-lppEN_run-1_desc-ICA_components.nii.gz\n",
      "INFO     io:writeresults:502 Writing denoised ICA coefficient feature set: /Users/taylor/Documents/datasets/ds003643/derivatives/tedana/sub-EN100/func/sub-EN100_task-lppEN_run-1/sub-EN100_task-lppEN_run-1_desc-ICAAccepted_components.nii.gz\n",
      "INFO     io:writeresults:508 Writing Z-normalized spatial component maps: /Users/taylor/Documents/datasets/ds003643/derivatives/tedana/sub-EN100/func/sub-EN100_task-lppEN_run-1/sub-EN100_task-lppEN_run-1_desc-ICAAccepted_stat-z_components.nii.gz\n",
      "INFO     tedana:tedana_workflow:889 Making figures folder with static component maps and timecourse plots.\n",
      "INFO     io:denoise_ts:374 Variance explained by decomposition: 95.75%\n",
      "/Users/taylor/Documents/tsalo/tedana/tedana/io.py:640: UserWarning: Data array used to create a new image contains 64-bit ints. This is likely due to creating the array with numpy and passing `int` as the `dtype`. Many tools such as FSL and SPM cannot deal with int64 in Nifti images, so for compatibility the data has been converted to int32.\n",
      "  nii = new_img_like(ref_img, newdata, affine=affine, copy_header=copy_header)\n",
      "INFO     tedana:tedana_workflow:918 Generating dynamic report\n",
      "INFO     tedana:tedana_workflow:921 Workflow completed\n"
     ]
    }
   ],
   "source": [
    "dset_dir = \"/Users/taylor/Documents/datasets/ds003643/\"\n",
    "deriv_dir = os.path.join(dset_dir, \"derivatives\")\n",
    "func_dir = os.path.join(deriv_dir, \"fmriprep/sub-EN100/func\")\n",
    "\n",
    "ECHO_TIMES = [12.8, 27.5, 43]  # hardcoded bc i'm lazy\n",
    "run_number = 1  # keeping as variable bc there are 10 runs. easy to loop over\n",
    "prefix = f\"sub-EN100_task-lppEN_run-{run_number}\"\n",
    "bold_files = [\n",
    "    os.path.join(func_dir,f\"{prefix}_echo-{echo + 1}_desc-preproc_bold.nii.gz\")\n",
    "    for echo in range(len(ECHO_TIMES))\n",
    "]\n",
    "confounds_file = os.path.join(func_dir,  f\"{prefix}_desc-confounds_timeseries.tsv\")\n",
    "mask_file = os.path.join(func_dir, f\"{prefix}_desc-brain_mask.nii.gz\")\n",
    "\n",
    "# Write tedana outputs to BIDS-like structure,\n",
    "# but use a separate folder for each run.\n",
    "tedana_out_dir = os.path.join(deriv_dir, \"tedana/sub-EN100/func\", prefix)\n",
    "os.makedirs(tedana_out_dir, exist_ok=True)\n",
    "# A folder for all of the shortened files.\n",
    "tedana_temp_dir = os.path.join(dset_dir, \"derivatives\", \"reduced_files\")\n",
    "os.makedirs(tedana_temp_dir, exist_ok=True)\n",
    "\n",
    "shortened_files, n_dummy_vols = drop_dummy_vols(\n",
    "    bold_files=bold_files,\n",
    "    confounds_file=confounds_file,\n",
    "    temp_dir=tedana_temp_dir,\n",
    ")\n",
    "\n",
    "tedana_workflow(\n",
    "    data=shortened_files,\n",
    "    tes=ECHO_TIMES,\n",
    "    out_dir=tedana_out_dir,\n",
    "    mask=mask_file,\n",
    "    prefix=prefix,\n",
    "    fittype=\"curvefit\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Label tedana components and fill in dummy volumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load mixing matrix and classifications from tedana\n",
    "mixing_matrix = os.path.join(tedana_out_dir, f\"{prefix}_desc-ICA_mixing.tsv\")\n",
    "metrics_df = os.path.join(tedana_out_dir, f\"{prefix}_desc-tedana_metrics.tsv\")\n",
    "mixing_matrix = pd.read_table(mixing_matrix)\n",
    "metrics_df = pd.read_table(metrics_df)\n",
    "\n",
    "# Prepend \"signal__\" to all accepted components' column names\n",
    "accepted_columns = metrics_df.loc[metrics_df[\"classification\"] != \"rejected\", \"Component\"]\n",
    "mixing_matrix = mixing_matrix.rename(columns={c: f\"signal__{c}\" for c in accepted_columns})\n",
    "\n",
    "# Add dummyvols back in to beginning of matrix\n",
    "mixing_matrix_data = mixing_matrix.to_numpy()\n",
    "first_row = mixing_matrix_data[0, :]\n",
    "leading_rows = np.ones((n_dummy_vols, mixing_matrix.shape[1])) * first_row\n",
    "new_mixing_matrix_arr = np.vstack((leading_rows, mixing_matrix_data))\n",
    "new_mixing_matrix = pd.DataFrame(new_mixing_matrix_arr, columns=mixing_matrix.columns)\n",
    "\n",
    "# Write out to custom confounds folder\n",
    "custom_confounds_folder = os.path.join(deriv_dir, \"custom_confounds_for_xcpd\")\n",
    "os.makedirs(custom_confounds_folder, exist_ok=True)\n",
    "# use the same name as the fMRIPrep confounds, but in the new folder\n",
    "custom_confounds_file = os.path.join(custom_confounds_folder, os.path.basename(confounds_file))\n",
    "new_mixing_matrix.to_csv(custom_confounds_file, sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Run XCPD with tedana-derived custom confounds\n",
    "\n",
    "Note: dummy-scans must match between tedana and xcpd\n",
    "```bash\n",
    "docker run --rm -u $(id -u) \\\n",
    "    -v /Users/taylor/Documents/datasets/ds003643:/bids-input:rw \\\n",
    "    -v /Users/taylor/Documents/tsalo/xcp_d/xcp_d:/usr/local/miniconda/lib/python3.8/site-packages/xcp_d \\\n",
    "    -v /Users/taylor/Documents/tsalo/xcp_d_testing/data/license.txt:/license.txt --env FS_LICENSE=/license.txt \\\n",
    "    pennlinc/xcp_d:unstable \\\n",
    "    /bids-input/derivatives/fmriprep \\\n",
    "    /bids-input/derivatives \\\n",
    "    participant \\\n",
    "    -w /bids-input/derivatives/work \\\n",
    "    --participant_label EN100 \\\n",
    "    --nuisance-regressors 27P \\\n",
    "    --custom_confounds /bids-input/derivatives/custom_confounds_for_xcpd \\\n",
    "    --dummy-scans auto \\\n",
    "    --bids-filter-file /bids-input/derivatives/code/filter_file.json \\\n",
    "    -vvv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
